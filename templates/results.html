<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Style Transfer</title>

    <!-- Bootstrap core CSS -->
    <link href="/static/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="/static/vendor/fontawesome-free/css/all.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Varela+Round" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Nunito:200,200i,300,300i,400,400i,600,600i,700,700i,800,800i,900,900i"
          rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="/static/css/grayscale.css" rel="stylesheet">

    <link href="/static/css/image-picker.css" rel="stylesheet">

    <!-- croppie-->
    <link rel="stylesheet" href="/static/css/croppie.css" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>

</head>

<style>
    a {
        font-weight: 900;
    }
    #header {
        background-image: url("../static/img/background2.jpg");
        background-repeat: no-repeat;
        background-attachment: fixed;
        background-position: center;
    }
    #restart-btn {
        background-color: white;
    }
    label.cabinet{
        display: block;
        cursor: pointer;
    }

    label.cabinet input.file{
        position: relative;
        height: 100%;
        width: auto;
        opacity: 0;
        -moz-opacity: 0;
        filter:progid:DXImageTransform.Microsoft.Alpha(opacity=0);
        margin-top:-30px;
    }

    figure figcaption {
        position: absolute;
        bottom: 0;
        color: #fff;
        width: 100%;
        padding-left: 9px;
        padding-bottom: 5px;
        text-shadow: 0 0 10px #000;
    }

    .fit {
      max-width: 50%;
      max-height: 50%;
    }

</style>


<body id="page-top">

<!-- Navigation -->
<!-- Navigation -->
<nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
    <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="/">Style Transfer</a>
    </div>
</nav>
<!-- Header -->
<header id="header" class="masthead" style="display: block;">
    <div class="container d-flex h-100 align-items-center">
        <div class="mx-auto text-center">
            <h1 class="mx-auto my-0 text-uppercase"><strong>Transfer Completed</strong></h1>
            <a href="{{ result_name }}" download>
                <img class="fit image_picker_image" src="{{ result_name }}" style="border:5px solid black">
                <br>
                <br>
                <button class="btn btn-primary mx-auto">Download</button>
            </a>
            <span>
                <a id="restart-btn" class="btn btn-default" href="/">Restart</a>
            </span>
        </div>
    </div>
</header>

{#<!-- Motivation Section -->#}
{#<section id="motivation" class="info-section text-center style2">#}
{#    <div class="container">#}
{#        <div class="row align-items-center no-gutters mb-4 mb-lg-5">#}
{#            <div class="col-xl-11 col-xl-11">#}
{#                <div class="text-center text-lg-left">#}
{#                    <h1 class ="text-white">Motivation</h1>#}
{#                    <br>#}
{#                    <p class="text-white-50 mb-0 style2-font">Motivated by the skills in fine art that people transfer the contents perceived into paintings with artistic style, we implemented an algorithm that combines the style of a painting with the content of a photograph to create a new artistic image in this project. Based on the fact that human painting with certain style costs a large amount of time, we tried to find an optimized way in which the machine, instead of humans, “learns” the style. We want to fuse the learned artistic style with a random object picture in order to help save the observation time spent on learning styles for artists and give people who do not have the painting skill a chance to create their own art pieces. </p>#}
{#                </div>#}
{#            </div>#}
{#        </div>#}
{#    </div>#}
{#</section>#}
{##}
{#<!-- Approach Section -->#}
{#<section id="approach" class="info-section text-center style1">#}
{#    <div class="container">#}
{#        <div class="row align-items-center no-gutters mb-4 mb-lg-5">#}
{#            <div class="col-xl-11 col-xl-11">#}
{#                <div class="text-center text-lg-left">#}
{#                    <h1>Approach</h1>#}
{#                    <br>#}
{#                    <p class="text-black-50 mb-0">Our approach is based on the suggestions of <a href="https://arxiv.org/abs/1508.06576" target="_blank">Gatys et al(2015)</a>: to utilize a multi-layer neural network through optimizing the content loss and style loss of a noisy image referring to an artistic style picture and a photographic content picture.</p>#}
{#                    <br>#}
{#                    <p class="text-black-50 mb-0">The next step is to reduce the possible time consumed on optimizing the loss by keeping iterating inside the multilayer convoluted neural network. To do this, we refer to the idea of <a href="https://arxiv.org/abs/1603.03417" target="_blank">Ulyanov et al(2015)</a> that training a feed-forward neural network to realize the faster style-transfer. This network made use of plentiful content images to approach the idealistic style and content loss, thus, we can generate the target image faster with the training process already done.</p>#}
{#                    <br>#}
{#                    <p class="text-black-50 mb-0">To further improve the efficiency, based on the VGG network implemented by <a href="https://arxiv.org/abs/1603.08155" target="_blank">Johnson et al(2016)</a> which recognizes the objects in an image, we came out with a possible approach to extract features from the style image.</p>#}
{#                    <br>#}
{#                    <p class="text-black-50 mb-0">After that, we want to do some improvement on practical functionalities, such as multi-style transfer and partial transfer. For multi-style transfer, we designed to assign different loss functions to different styles in order to fuse two or more styles. As for the partial transfer, one possible solution we came up with is to separate the background from the image using existing segmentation and detection algorithms such as  the K-means clustering method. </p>#}
{#                </div>#}
{#            </div>#}
{#        </div>#}
{#    </div>#}
{#</section>#}
{##}
{##}
{##}
{#<!-- Implementation Section -->#}
{#<section id="implementation" class="info-section text-center style2">#}
{#    <div class="container">#}
{#        <div class="row align-items-center no-gutters mb-4 mb-lg-5">#}
{#            <div class="col-xl-11 col-xl-11">#}
{#                <div class="text-center text-lg-left">#}
{#                    <h1 class ="text-white">Implementation</h1>#}
{#                    <br>#}
{#                    <p class="text-white-50 mb-0 style2-font">The training and classifying structures in the implementation of process will be transitional as others since the build-up of CNN seems straightforward in hindsight, but we retune the structure in light of the genres of styles we use, thus construct our models with different structure with changes in the order and repetition of implementation, modifying the numbers of layers of the style, step of iterations we go through to come out with a new solution to reproduce the picture with different visual effect of style transfer. We built our neural network with the structure of VGG network and trained this with the feed-forward methodology. </p>#}
{#                    <br>#}
{#                    <p class="text-white-50 mb-0 style2-font">Higher layers in the convolutional neural network capture the more detailed content, but in the reconstruction, they do not constrain the pixel locations. To obtain the style and apply it appropriately throughout the whole content, with VGG network we tried different filters to extract the spatial features and then combined the content and style. By doing this, we define different loss functions to train the data for classification and more customized weight for the target visual effect. With trained abstract style from our model, we then match and combine the style with content image. To adjust the effect of the style to achieve perceptual optimization, we change the weight of style loss in the following total loss function: </p>#}
{#                    <br>#}
{#                    <p class="text-white mb-0 style2-font"  align="middle"><b>Ltotal(p, a, x) = αLcontent(p, x) + βLstyle(a, x)</b></p>#}
{#                    <br>#}
{#                    <p class="text-white-50 mb-0 style2-font">Specifically, when we want to yield a strongly stylistic picture, we increase α/β and vice versa. Thus, a large part of modification in our approach lies on the weight of original contents and share of styles. Considering the situation where sometimes layer optimized result does not necessarily match the appreciation in the biological evaluation, our approach customized the layers to produce a version that human beings feel most aesthetic beauty.</p>#}
{#                    <br>#}
{#                    <p class="text-white-50 mb-0 style2-font">We trained each of style with 80000 images for 2 epoch in Coco dataset. To speed up the training process, we rent a computing virtual machine on Google Cloud whose GPU can support us to train one style with ten thousand content images within 8 hours.</p>#}
{#                    <br>#}
{#                    <p class="text-white-50 mb-0 style2-font">After the experiments on existing solutions and the our implementation of feed-forward methodology, we worked on some improvements on practical functionalities:  </p>#}
{#                    <br>#}
{#                     <p class="text-white-50 mb-0 style2-font">1.  fuse/transfer more than one style</p>#}
{#                     <br>#}
{#                     <p class="text-white-50 mb-0 style2-font">We computed an idealistic style loss for each style then combine them by assigning weights for each style (in our practice is 1:1). Another possible approach we thought of is to do the style transfer training multiple times using the style images as inputs to train one combined style image, then use this combined style image and the content image as inputs of the feed-forward neural network to compute our desired styled image.</p>#}
{#                     <br>#}
{#                     <p class="text-white-50 mb-0 style2-font">2. realize the function of partial style transferring such as background style transfer</p>#}
{#                     <br>#}
{#                     <p class="text-white-50 mb-0 style2-font">The first step we did is to accomplish background segregation for the user input image.  The target for transfer can be various, such as foreground style transfer, which means we only make style transfer for foreground picture but leave background picture as what it is. For faster transfer, we crop the user image image to roughly split the foreground and background before detailed separation.</p>#}
{#                     <br>#}
{#                     <p class="text-white-50 mb-0 style2-font">For detailed separation within the rectangle, we implemented the interactive foreground extraction using GrabCut algorithm (designed by Carsten Rother), which used a Gaussian Mixture Model to realize clustering. It redistributes of pixels based on the weights learnt from given data in every iteration. Then it applies a min-cut algorithm to separate the graph, and repeats until the classification converges. After separation, we apply only the foreground, or background part to style transfer then combine it back to the original image.</p>#}
{#                     <br>#}
{#                     <p class="text-white-50 mb-0 style2-font">We finalize our implementation by evaluation on all of the resulting pictures we get with different parameters. We evaluated the performance of our solution basically on its visual results and speed. For evaluation, we tested our network on transferring of different content and style images to check whether it could generate images with the details in the content image and features in the style image while producing results that human beings feel most aesthetic beauty. For the speed, we tested our method both on CPU and GPU with images of different resolution to check the time it needs to finish the task, and compared that with the time needed for training on Google Cloud computing machine. We also compared our results with the results generated by some well-known mobile applications such as Prisma, Artify and Picsart. </p>#}
{#                     <br>#}
{#                     <div>#}
{#                     <p class="text-white-50 mb-0 style2-font">Check out our implementation on Github:&nbsp&nbsp#}
{#                        <a href="https://github.com/overflocat/fast-neural-style-keras" target="_blank">#}
{#                            <img class="img-fluid" src="../static/img/github-white.png" alt="github">#}
{#                        </a>#}
{#                    </p>#}
{#                    </div>#}
{#                </div>#}
{#            </div>#}
{#        </div>#}
{#    </div>#}
{#</section>#}
{##}
{#<!-- Results Section -->#}
{#<section id="results" class="info-section text-center style1">#}
{#    <div class="container">#}
{#        <div class="row align-items-center no-gutters mb-4 mb-lg-5">#}
{#            <div class="col-xl-11 col-xl-11">#}
{#                <div class="text-center text-lg-left">#}
{#                    <h1>Results</h1>#}
{#                    <br>#}
{#                    <p class="text-black-50 mb-0">It took about 8 hours to train a network on a Nvidia K80 GPU. After training, predicting is really fast and only uses 1 to 5 seconds to generate an output image given pre-trained style model.</p>#}
{#                    <br>#}
{#                    <p class="text-black-50 mb-0">Moreover, our approach improved control in the emphasis of style and the content which means that we determined the weight of styles which perform the best according to user expectation when experimenting with different data. We also provided some new ideas for making the style-transfer more user-friendly and with more function for users to create their own artwork, such as achieving background separation with rendering up to two styles on the foreground and background respectively. For these tasks, we also checked the efficiency of our algorithm and the results to make sure that the results are artistically meaningful.</p>#}
{#                    <br>#}
{#                    <p class="text-black-50 mb-0">As a real-time demonstration for the style transfer we implement, we provided an online platform which allows users to upload their own content images and choose from pre-trained style images to generate their own artwork, with extra functions which are mentioned above.</p>#}
{#                    <br>#}
{#                    <p class="text-black-50 mb-0">Here are some results of single style transfer:</p>#}
{#                    <br>#}
{#                    <img class="img-fluid" src="../static/img/UWfootball.jpg" alt="">#}
{#                    <text> &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp</text>#}
{#                    <img class="img-fluid" src="../static/img/2.jpg" alt="">#}
{#                    <text> &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp</text>#}
{#                    <img class="img-fluid" src="../static/img/UWfootball_udnie.jpg" alt="">#}
{#                    <br>#}
{#                    <br>#}
{#                    <img class="img-fluid" src="../static/img/AlumniPark.jpg" alt="">#}
{#                    <text> &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp</text>#}
{#                    <img class="img-fluid" src="../static/img/3.png" alt="">#}
{#                    <text> &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp</text>#}
{#                    <img class="img-fluid" src="../static/img/AlumniPark_output.jpg" alt="">#}
{#                    <br>#}
{#                    <br>#}
{#                    <p class="text-black-50 mb-0">Here is a result of combining two styles:</p>#}
{#                    <br>#}
{#                    <img class="img-fluid" src="../static/img/commence.jpg" alt="">#}
{#                    <text> &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp</text>#}
{#                    <img class="img-fluid" src="../static/img/combine_style.jpg" alt="">#}
{#                    <text> &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp</text>#}
{#                    <img class="img-fluid" src="../static/img/commence_combine.jpg" alt="">#}
{#                    <br>#}
{#                    <br>#}
{#                    <p class="text-black-50 mb-0">Here is a result of partial transfer:</p>#}
{#                    <br>#}
{#                    <img class="img-fluid" src="../static/img/partial_transfer.jpg" alt="">#}
{#                    <text> &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp</text>#}
{#                    <img class="img-fluid" src="../static/img/partial_transfer_output.jpg" alt="">#}
{#                </div>#}
{#            </div>#}
{#        </div>#}
{#    </div>#}
{#</section>#}
{##}
{#<!-- Discussion Section -->#}
{#<section id="discussion" class="info-section text-center style2">#}
{#    <div class="container">#}
{#        <div class="row align-items-center no-gutters mb-4 mb-lg-5">#}
{#            <div class="col-xl-11 col-xl-11">#}
{#                <div class="text-center text-lg-left">#}
{#                    <h1 class ="text-white">Discussion.</h1>#}
{#                    <br>#}
{#                    <p class="text-white-50 mb-0 style2-font">Although the time for generating the stylistic image reduced, the cost for learning those styles is still expensive. One drawback of feed-forward style transfer is that we need to train every style we want to transfer with extensively. In other words, in order to generate a new style, we have to spend hours to train it with a great bunch of pictures.</p>#}
{#                    <br>#}
{#                    <p class="text-white-50 mb-0 style2-font">Currently, the top-notch choice is to use style swap. Instead of optimizing a noisy feature map, we swap the fixed size of patches between style image and content image, where the style patches match content patch best. Through the process, we can construct a swapped patch map with both content and style feature. By decoding the swapped patch map in a trained inverse network, we can transfer any styles to our content image at a decent speed.</p>#}
{#                    <br>#}
{#                    <p class="text-white-50 mb-0 style2-font">We also found problems in partial transformation. This algorithm separate different color regions, which worked pretty well for pictures whose foreground and background are highly color-contrasted. Moreover, we tried to segment foreground by calculating the local standard deviation of the image. It works well for pictures who have an outstanding focused region. As the edges of the focused region are sharper than the defocused region, we can filter the image by a threshold value for local standard deviation. At the same time, the implemented algorithm for background separation has a high demand for the input image, such as the color contrast and the focus. It comes out that the cases where images are not well separated do exist We currently do not have a solution for that based on the algorithm we used, but there are typical cases where the approach works well when we have required user images.</p>#}
{##}
{#                </div>#}
{#            </div>#}
{#        </div>#}
{#    </div>#}
{#</section>#}

<!-- Footer -->
<footer class="bg-black small text-center text-white-50">
          <div class="container github-bt">
            <a href="https://github.com/overflocat/fast-neural-style-keras" target="_blank">
                <img class="img-fluid" src="../static/img/github-white.png" alt="">
            </a>
          </div>
    <div class="container">
        Website Made by Yue Sun.
        <br>
        Project by Yue Sun, Yufei Wang, Wei Xie, Fei Gu.
    </div>
</footer>

<!-- Bootstrap core JavaScript -->
<script src="/static/vendor/jquery/jquery.min.js"></script>
<script src="/static/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

<!-- Plugin JavaScript -->
<script src="/static/vendor/jquery-easing/jquery.easing.min.js"></script>
<!-- Scripts for the Image Picker -->
<script src="/static/js/image-picker.js"></script>

<!-- Custom scripts for this template -->
<script src="/static/js/grayscale.js"></script>

<script src="/static/js/croppie.js"></script>

</body>

</html>
